from dotenv import load_dotenv
load_dotenv()
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
import streamlit as st
import tempfile
import os

st.title("ğŸ“„ ChatPDF")
st.write("PDFë¥¼ ì—…ë¡œë“œí•˜ê³  ì§ˆë¬¸í•´ë³´ì„¸ìš”!")

uploaded_file = st.file_uploader("PDF íŒŒì¼ ì„ íƒ", type="pdf")

def process_pdf(uploaded_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        loader = PyPDFLoader(tmp_file.name)
        pages = loader.load_and_split()
    os.unlink(tmp_file.name)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
    return pages

if uploaded_file:
    with st.spinner('PDF ì²˜ë¦¬ ì¤‘...'):
        # PDF ì²˜ë¦¬ ë° ë²¡í„° DB ìƒì„±
        pages = process_pdf(uploaded_file)
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)
        texts = text_splitter.split_documents(pages)
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-mpnet-base-v2")
        db = Chroma.from_documents(texts, embeddings)
        
    st.success(f"âœ… PDF ì²˜ë¦¬ ì™„ë£Œ! ({len(texts)}ê°œ ë¬¸ì„œ ì¡°ê°)")
    
    # ì§ˆë¬¸ ì…ë ¥
    question = st.text_input('ğŸ’¬ PDFì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”:')
    
    if st.button('ğŸš€ ì§ˆë¬¸í•˜ê¸°', type="primary"):
        if question:
            with st.spinner('ë‹µë³€ ìƒì„± ì¤‘...'):
                try:
                    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
                    qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())
                    result = qa_chain.run(question)
                    st.markdown(f"### ğŸ¤– ë‹µë³€:\n{result}")
                except Exception as e:
                    st.error(f"âŒ OpenAI API ì˜¤ë¥˜: {e}")
                    st.info("ğŸ” ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼:")
                    docs = db.similarity_search(question, k=3)
                    for i, doc in enumerate(docs, 1):
                        with st.expander(f"ğŸ“„ ë¬¸ì„œ {i}"):
                            st.write(doc.page_content)
        else:
            st.warning("âš ï¸ ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
else:
    st.info("ğŸ‘† PDF íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ì‹œì‘ë©ë‹ˆë‹¤!")