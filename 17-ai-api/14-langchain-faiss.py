from dotenv import load_dotenv
load_dotenv()

from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# ìƒˆë¡œìš´ íŒ¨í‚¤ì§€ ì‚¬ìš© (ê²½ê³  í•´ê²°)
try:
    from langchain_huggingface import HuggingFaceEmbeddings
    print("âœ… ìƒˆë¡œìš´ langchain-huggingface íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤")
except ImportError:
    from langchain_community.embeddings import HuggingFaceEmbeddings
    print("âš ï¸  ê¸°ì¡´ íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤ (deprecated)")

from langchain_community.vectorstores import Chroma, FAISS
import os
import time

def create_vector_database(pdf_path="./17-ai-api/resume.pdf", persist_directory="./chroma_db", use_faiss=False):
    """PDF ë¬¸ì„œë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” í•¨ìˆ˜"""
    
    try:
        # PDF Loader
        print(f"ğŸ“„ PDF ë¡œë”© ì¤‘: {pdf_path}")
        loader = PyPDFLoader(pdf_path)
        pages = loader.load_and_split()
        print(f"âœ… {len(pages)} í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")

        # Text Split (ë” ê°€ë²¼ìš´ ì„¤ì •)
        print("ğŸ”ª í…ìŠ¤íŠ¸ ë¶„í•  ì¤‘...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=200,  # 300 -> 200ìœ¼ë¡œ ì¶•ì†Œ
            chunk_overlap=10,  # 20 -> 10ìœ¼ë¡œ ì¶•ì†Œ
            length_function=len,
            is_separator_regex=False,
        )
        texts = text_splitter.split_documents(pages)
        print(f"âœ… {len(texts)} ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ ìƒì„±")

        # Embeddings (ê°€ë²¼ìš´ ëª¨ë¸ ì‚¬ìš©)
        print("ğŸ§  ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embeddings_model = HuggingFaceEmbeddings(
            model_name="sentence-transformers/all-MiniLM-L6-v2",  # ë” ê°€ë²¼ìš´ ëª¨ë¸
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
        print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        if use_faiss:
            print("ğŸ’¾ FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
            start_time = time.time()
            db = FAISS.from_documents(texts, embeddings_model)
            end_time = time.time()
            print(f"âœ… FAISS ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            
            # FAISS ì €ì¥ (ì„ íƒì )
            faiss_path = persist_directory.replace("chroma_db", "faiss_db")
            db.save_local(faiss_path)
            print(f"âœ… FAISS ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ: {faiss_path}")
            
        else:
            print("ğŸ’¾ ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì¤‘...")
            
            # ChromaDB ì•ˆì „í•œ ìƒì„± ë°©ë²•
            try:
                # ë°©ë²• 1: ë©”ëª¨ë¦¬ì—ì„œ ë¨¼ì € ìƒì„±
                print("ğŸ”„ ë©”ëª¨ë¦¬ì—ì„œ ChromaDB ìƒì„± ì¤‘...")
                start_time = time.time()
                db_temp = Chroma.from_documents(texts, embeddings_model)
                end_time = time.time()
                print(f"âœ… ë©”ëª¨ë¦¬ ChromaDB ìƒì„± ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
                
                # ë°©ë²• 2: ì˜êµ¬ ì €ì¥ì†Œë¡œ ë³µì‚¬
                print(f"ğŸ’¾ ì˜êµ¬ ì €ì¥ì†Œë¡œ ë³µì‚¬ ì¤‘: {persist_directory}")
                if os.path.exists(persist_directory):
                    import shutil
                    shutil.rmtree(persist_directory)
                    print("ğŸ—‘ï¸  ê¸°ì¡´ ë°ì´í„°ë² ì´ìŠ¤ ì‚­ì œ")
                
                start_time = time.time()
                db = Chroma.from_documents(
                    texts, 
                    embeddings_model,
                    persist_directory=persist_directory
                )
                end_time = time.time()
                print(f"âœ… ChromaDB ì˜êµ¬ ì €ì¥ ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
                
            except Exception as chroma_error:
                print(f"âŒ ChromaDB ìƒì„± ì‹¤íŒ¨: {chroma_error}")
                print("ğŸ”„ FAISSë¡œ ëŒ€ì²´...")
                start_time = time.time()
                db = FAISS.from_documents(texts, embeddings_model)
                end_time = time.time()
                print(f"âœ… FAISS ëŒ€ì²´ ìƒì„± ì™„ë£Œ! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        
        print(f"âœ… ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ì™„ë£Œ")
        return db, embeddings_model
        
    except Exception as e:
        print(f"âŒ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def search_documents(db, query, k=3):
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
    if db is None:
        print("âŒ ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
        return []
    
    try:
        print(f"ğŸ” ê²€ìƒ‰ ì¤‘: '{query}'")
        start_time = time.time()
        results = db.similarity_search(query, k=k)
        end_time = time.time()
        
        print(f"ğŸ“‹ {len(results)} ê°œì˜ ê´€ë ¨ ë¬¸ì„œ ë°œê²¬! ì†Œìš”ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        for i, doc in enumerate(results, 1):
            print(f"\n--- ê²°ê³¼ {i} ---")
            print(f"ë‚´ìš©: {doc.page_content[:150]}...")
            if hasattr(doc, 'metadata') and doc.metadata:
                print(f"ë©”íƒ€ë°ì´í„°: {doc.metadata}")
        
        return results
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

def load_existing_database(persist_directory="./chroma_db", use_faiss=False):
    """ê¸°ì¡´ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
    
    if use_faiss:
        faiss_path = persist_directory.replace("chroma_db", "faiss_db")
        if not os.path.exists(faiss_path):
            print(f"âŒ FAISS ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {faiss_path}")
            return None, None
        
        try:
            embeddings_model = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            db = FAISS.load_local(faiss_path, embeddings_model, allow_dangerous_deserialization=True)
            print(f"âœ… ê¸°ì¡´ FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ: {faiss_path}")
            return db, embeddings_model
        except Exception as e:
            print(f"âŒ FAISS ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None, None
    
    else:
        if not os.path.exists(persist_directory):
            print(f"âŒ ChromaDBê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {persist_directory}")
            return None, None
        
        try:
            embeddings_model = HuggingFaceEmbeddings(
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                model_kwargs={'device': 'cpu'},
                encode_kwargs={'normalize_embeddings': True}
            )
            db = Chroma(persist_directory=persist_directory, embedding_function=embeddings_model)
            print(f"âœ… ê¸°ì¡´ ChromaDB ë¡œë“œ ì™„ë£Œ: {persist_directory}")
            return db, embeddings_model
        except Exception as e:
            print(f"âŒ ChromaDB ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None, None

# ë©”ì¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸš€ PDF ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*60)
    
    # FAISS ìš°ì„  ì‹œë„ (ë” ì•ˆì •ì )
    print("ğŸ“‹ FAISS ë°©ì‹ìœ¼ë¡œ ì‹œë„...")
    db, embeddings = create_vector_database(use_faiss=True)
    
    if db is None:
        print("ğŸ”„ ChromaDB ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")
        db, embeddings = create_vector_database(use_faiss=False)
    
    if db is not None:
        # ìƒ˜í”Œ ê²€ìƒ‰
        sample_queries = [
            "ê²½ë ¥",
            "ê¸°ìˆ  ìŠ¤íƒ", 
            "í”„ë¡œì íŠ¸",
            "êµìœ¡"
        ]
        
        print("\n" + "="*60)
        print("ğŸ” ìƒ˜í”Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        print("="*60)
        
        for query in sample_queries[:2]:  # ì²˜ìŒ 2ê°œë§Œ í…ŒìŠ¤íŠ¸
            print(f"\n{'='*40}")
            search_documents(db, query, k=2)
            
        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
    
    else:
        print("âŒ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")