# ex08.py
# https://www.hankyung.com/globalmarket/news-globalmarket
# í•œê²½ê¸€ë¡œë²Œ ë§ˆì¼“ì˜ ë‰´ìŠ¤ ê¸°ì‚¬ íƒ€ì´í‹€ 10ê°œ / image urlì„ ì¶œë ¥í•˜ì‹œì˜¤.

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time

def crawl_hankyung_news():
    """í•œêµ­ê²½ì œ ê¸€ë¡œë²Œë§ˆì¼“ ë‰´ìŠ¤ í¬ë¡¤ë§ - XPath ì‚¬ìš©"""
    
    # Chrome ì„¤ì • (Selenium Manager ì‚¬ìš© - ìë™ìœ¼ë¡œ chromedriver ê´€ë¦¬)
    options = Options()
    options.add_argument('--headless')  # ë¸Œë¼ìš°ì € ì°½ ì—†ì´ ì‹¤í–‰
    options.add_argument('--disable-gpu')
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")

    # ë“œë¼ì´ë²„ ì‹¤í–‰ (ê²½ë¡œ ì§€ì • ì—†ì´ ìë™ ê´€ë¦¬)
    driver = webdriver.Chrome(options=options)

    try:
        print("ğŸš€ í•œêµ­ê²½ì œ ê¸€ë¡œë²Œë§ˆì¼“ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘... (XPath ì‚¬ìš©)")
        
        # íƒ€ê²Ÿ URL ì ‘ì†
        url = 'https://www.hankyung.com/globalmarket/news-globalmarket'
        print(f"ğŸ“° ì ‘ì† ì¤‘: {url}")
        driver.get(url)
        time.sleep(3)  # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        print(f"âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ: {driver.title}")
        
        # ë‹¤ì–‘í•œ ë‰´ìŠ¤ XPath ì„ íƒìë“¤ ì‹œë„
        xpath_selectors = [
            # ê¸°ë³¸ ë‰´ìŠ¤ ë§í¬ë“¤
            "//ul[@class='list_basic']//li//a",                    # ê¸°ì¡´ ì„ íƒìì˜ XPath ë²„ì „
            "//div[contains(@class, 'news-list')]//a",             # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë§í¬
            "//article//a",                                        # ê¸°ì‚¬ ë‚´ ëª¨ë“  ë§í¬
            
            # ì œëª© ê¸°ë°˜ ì„ íƒìë“¤
            "//h3//a",                                             # h3 íƒœê·¸ ë‚´ ë§í¬
            "//h4//a",                                             # h4 íƒœê·¸ ë‚´ ë§í¬
            "//h5//a",                                             # h5 íƒœê·¸ ë‚´ ë§í¬
            
            # í´ë˜ìŠ¤ ê¸°ë°˜ ì„ íƒìë“¤
            "//div[contains(@class, 'headline')]//a",              # í—¤ë“œë¼ì¸ í´ë˜ìŠ¤
            "//div[contains(@class, 'title')]//a",                 # ì œëª© í´ë˜ìŠ¤
            "//span[contains(@class, 'title')]//a",                # span ì œëª© í´ë˜ìŠ¤
            "//p[contains(@class, 'title')]//a",                   # p ì œëª© í´ë˜ìŠ¤
            
            # href ì†ì„± ê¸°ë°˜ ì„ íƒìë“¤ (ë” ì •í™•í•œ ë‰´ìŠ¤ ë§í¬)
            "//a[contains(@href, '/article/')]",                   # article í¬í•¨ ë§í¬
            "//a[contains(@href, '/news/')]",                      # news í¬í•¨ ë§í¬
            "//a[contains(@href, 'hankyung.com')]",               # í•œê²½ ë„ë©”ì¸ ë§í¬
            
            # í…ìŠ¤íŠ¸ ê¸¸ì´ ê¸°ë°˜ ì„ íƒìë“¤
            "//a[string-length(text()) > 10]",                    # 10ì ì´ìƒ í…ìŠ¤íŠ¸ ë§í¬
            "//a[string-length(normalize-space(text())) > 5]",    # 5ì ì´ìƒ ì •ê·œí™”ëœ í…ìŠ¤íŠ¸
            
            # ë³µí•© ì¡°ê±´ ì„ íƒìë“¤
            "//a[contains(@href, 'article') and string-length(text()) > 5]",  # article URL + ê¸´ í…ìŠ¤íŠ¸
            "//div[contains(@class, 'news') or contains(@class, 'article')]//a",  # ë‰´ìŠ¤/ê¸°ì‚¬ í´ë˜ìŠ¤ í•˜ìœ„
            
            # ìƒìœ„ ìš”ì†Œ ê¸°ë°˜ ì„ íƒìë“¤
            "//li//a[contains(@href, 'article')]",                # li í•˜ìœ„ì˜ article ë§í¬
            "//section//a[contains(@href, 'article')]",           # section í•˜ìœ„ì˜ article ë§í¬
            "//main//a[contains(@href, 'article')]",              # main í•˜ìœ„ì˜ article ë§í¬
        ]
        
        news_found = False
        all_links = []
        
        for i, xpath in enumerate(xpath_selectors):
            try:
                print(f"ğŸ” XPath {i+1} ì‹œë„: {xpath}")
                news_items = driver.find_elements(By.XPATH, xpath)
                print(f"   â†’ {len(news_items)}ê°œ ìš”ì†Œ ë°œê²¬")
                
                if news_items:
                    all_links.extend(news_items)
                    
                    # ì²˜ìŒ ëª‡ ê°œ ë§í¬ ì •ë³´ ë¯¸ë¦¬ë³´ê¸°
                    if i < 3 and len(news_items) > 0:
                        print(f"   ğŸ“ ìƒ˜í”Œ: {news_items[0].text[:30]}...")
                    
            except Exception as e:
                print(f"   âŒ XPath ì‹¤íŒ¨: {str(e)[:50]}...")
                
        # ì¤‘ë³µ ì œê±° ë° ìœ íš¨ì„± ê²€ì¦
        seen_urls = set()
        unique_links = []
        
        print(f"ğŸ”„ ì¤‘ë³µ ì œê±° ë° ìœ íš¨ì„± ê²€ì¦ ì¤‘...")
        
        for link in all_links:
            try:
                href = link.get_attribute('href')
                text = link.text.strip()
                
                # ë” ì—„ê²©í•œ ìœ íš¨ì„± ê²€ì‚¬
                if (href and 
                    href not in seen_urls and 
                    text and 
                    len(text) > 3 and
                    ('hankyung.com' in href or 'article' in href)):
                    
                    seen_urls.add(href)
                    unique_links.append(link)
                    
            except Exception as e:
                continue
                
        print(f"ğŸ“Š ì´ {len(unique_links)}ê°œ ê³ ìœ  ìœ íš¨ ë§í¬ ë°œê²¬")
        
        # ë‰´ìŠ¤ ì •ë³´ ì¶”ì¶œ ë° ì¶œë ¥
        news_count = 0
        print(f"\n{'='*80}")
        print(f"ğŸ—ï¸  ìˆ˜ì§‘ëœ ë‰´ìŠ¤ ëª©ë¡ (XPath ì‚¬ìš©)")
        print(f"{'='*80}")
        
        for i, item in enumerate(unique_links[:15]):  # ìƒìœ„ 15ê°œ
            try:
                title = item.text.strip()
                link = item.get_attribute('href')
                
                # ìµœì¢… ë‰´ìŠ¤ í•„í„°ë§
                if (title and link and 
                    len(title) > 5 and 
                    any(keyword in link for keyword in ['article', 'news', 'hankyung'])):
                    
                    news_count += 1
                    
                    # ì œëª© ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°)
                    cleaned_title = ' '.join(title.split())
                    
                    print(f"\nğŸ“° [{news_count:2d}] {cleaned_title}")
                    print(f"ğŸ”— {link}")
                    print(f"ğŸ“ ì œëª© ê¸¸ì´: {len(cleaned_title)}ì")
                    print("-" * 80)
                    
                    news_found = True
                    
                    if news_count >= 12:  # ìµœëŒ€ 12ê°œê¹Œì§€
                        break
                        
            except Exception as e:
                print(f"âš ï¸ ë§í¬ ì²˜ë¦¬ ì‹¤íŒ¨ [{i+1}]: {str(e)[:50]}...")
                continue
        
        if not news_found:
            print("âŒ XPathë¡œ ë‰´ìŠ¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("ğŸ” í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ ì •ë³´:")
            
            # í˜ì´ì§€ì˜ ëª¨ë“  ë§í¬ ê°œìˆ˜ í™•ì¸
            all_a_tags = driver.find_elements(By.XPATH, "//a")
            print(f"   ğŸ“Š í˜ì´ì§€ ë‚´ ì „ì²´ <a> íƒœê·¸: {len(all_a_tags)}ê°œ")
            
            # href ì†ì„±ì´ ìˆëŠ” ë§í¬ ê°œìˆ˜
            links_with_href = driver.find_elements(By.XPATH, "//a[@href]")
            print(f"   ğŸ”— href ì†ì„±ì´ ìˆëŠ” ë§í¬: {len(links_with_href)}ê°œ")
            
            # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬ ê°œìˆ˜  
            links_with_text = driver.find_elements(By.XPATH, "//a[text()]")
            print(f"   ğŸ“ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ë§í¬: {len(links_with_text)}ê°œ")
            
        return news_count
        
    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0
        
    finally:
        # ë“œë¼ì´ë²„ ì¢…ë£Œ
        driver.quit()
        print(f"\nâœ… ë“œë¼ì´ë²„ ì¢…ë£Œ ì™„ë£Œ")

if __name__ == "__main__":
    print("ğŸ¯ XPathë¥¼ í™œìš©í•œ í•œêµ­ê²½ì œ ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘!")
    print("="*80)
    
    count = crawl_hankyung_news()
    
    print("="*80)
    print(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! ì´ {count}ê°œì˜ ë‰´ìŠ¤ë¥¼ ì„±ê³µì ìœ¼ë¡œ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤!")
    
    if count > 0:
        print("ğŸ’¡ XPath ì„ íƒìê°€ ì„±ê³µì ìœ¼ë¡œ ì‘ë™í–ˆìŠµë‹ˆë‹¤!")
    else:
        print("âš ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í˜ì´ì§€ êµ¬ì¡°ë¥¼ ë‹¤ì‹œ í™•ì¸í•´ë³´ì„¸ìš”.")