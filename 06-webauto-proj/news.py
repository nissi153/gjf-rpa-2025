import streamlit as st
import pandas as pd
import time
import re
from datetime import datetime
import requests
from bs4 import BeautifulSoup
import urllib.parse

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤í¬ë˜í¼",
    page_icon="ğŸ“°",
    layout="wide"
)

st.title("ğŸ“° ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ìŠ¤í¬ë˜í¼")
st.markdown("AI, í™˜ìœ¨, ë‚˜ìŠ¤ë‹¥ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ìë™ìœ¼ë¡œ ìˆ˜ì§‘í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.")

class YonhapNewsScraper:
    def __init__(self):
        self.base_url = "https://www.yna.co.kr"
        self.search_url = "https://www.yna.co.kr/search"
        self.rss_url = "https://www.yna.co.kr/rss/all.xml"
        self.naver_api_url = "https://openapi.naver.com/v1/search/news.json"
    
    def search_news_from_rss(self, keyword, max_articles=10):
        """RSS í”¼ë“œì—ì„œ í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ ê²€ìƒ‰"""
        articles = []
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(self.rss_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'xml')
            items = soup.find_all('item')
            
            for item in items:
                try:
                    title = item.find('title').text.strip()
                    link = item.find('link').text.strip()
                    pub_date = item.find('pubDate').text.strip() if item.find('pubDate') else datetime.now().strftime('%Y-%m-%d')
                    description = item.find('description').text.strip() if item.find('description') else f"{keyword} ê´€ë ¨ ë‰´ìŠ¤"
                    
                    # í‚¤ì›Œë“œê°€ ì œëª©ì´ë‚˜ ì„¤ëª…ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    if keyword.lower() in title.lower() or keyword.lower() in description.lower():
                        articles.append({
                            'keyword': keyword,
                            'date': pub_date,
                            'title': title,
                            'url': link,
                            'summary': description[:200] + "..." if len(description) > 200 else description
                        })
                        
                        if len(articles) >= max_articles:
                            break
                            
                except Exception as e:
                    continue
                    
        except Exception as e:
            st.warning(f"RSS í”¼ë“œì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return articles
    
    def search_news_from_google(self, keyword, max_articles=10):
        """êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ì—°í•©ë‰´ìŠ¤ ê¸°ì‚¬ ê²€ìƒ‰"""
        articles = []
        
        try:
            # êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰ URL (ì—°í•©ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í•œì •)
            query = f"{keyword} site:yna.co.kr"
            search_url = f"https://www.google.com/search?q={urllib.parse.quote(query)}&tbm=nws"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # êµ¬ê¸€ ë‰´ìŠ¤ ê²°ê³¼ íŒŒì‹±
            news_results = soup.find_all('div', class_='SoaBEf')
            
            for result in news_results[:max_articles]:
                try:
                    title_element = result.find('div', class_='MBeuO')
                    if title_element:
                        title = title_element.get_text().strip()
                        
                        link_element = result.find('a')
                        link = link_element.get('href') if link_element else ""
                        
                        date_element = result.find('span', class_='r0bn4c')
                        date_text = date_element.get_text().strip() if date_element else datetime.now().strftime('%Y-%m-%d')
                        
                        summary_element = result.find('div', class_='GI74Re')
                        summary = summary_element.get_text().strip() if summary_element else f"{keyword} ê´€ë ¨ ë‰´ìŠ¤"
                        
                        if title and 'yna.co.kr' in link:
                            articles.append({
                                'keyword': keyword,
                                'date': date_text,
                                'title': title,
                                'url': link,
                                'summary': summary[:200] + "..." if len(summary) > 200 else summary
                            })
                            
                except Exception as e:
                    continue
                    
        except Exception as e:
            st.warning(f"êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
        return articles
    
    def search_news(self, keyword, max_articles=10):
        """í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰ (ì—¬ëŸ¬ ë°©ë²• ì‹œë„)"""
        articles = []
        
        # ë°©ë²• 1: RSS í”¼ë“œì—ì„œ ê²€ìƒ‰
        st.info(f"RSS í”¼ë“œì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
        rss_articles = self.search_news_from_rss(keyword, max_articles)
        articles.extend(rss_articles)
        
        # ë°©ë²• 2: êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ê²€ìƒ‰ (RSSì—ì„œ ì¶©ë¶„í•˜ì§€ ì•Šì€ ê²½ìš°)
        if len(articles) < max_articles:
            st.info(f"êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ '{keyword}' ì¶”ê°€ ê²€ìƒ‰ ì¤‘...")
            remaining = max_articles - len(articles)
            google_articles = self.search_news_from_google(keyword, remaining)
            articles.extend(google_articles)
        
        # ë°©ë²• 3: ì§ì ‘ ì—°í•©ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ê²€ìƒ‰ (ë§ˆì§€ë§‰ ì‹œë„)
        if len(articles) < max_articles:
            st.info(f"ì—°í•©ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ '{keyword}' ì§ì ‘ ê²€ìƒ‰ ì¤‘...")
            remaining = max_articles - len(articles)
            direct_articles = self.search_news_direct(keyword, remaining)
            articles.extend(direct_articles)
        
        return articles[:max_articles]
    
    def search_news_direct(self, keyword, max_articles=10):
        """ì—°í•©ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ê²€ìƒ‰"""
        articles = []
        
        try:
            # ê²€ìƒ‰ URL êµ¬ì„±
            search_url = f"{self.search_url}?query={urllib.parse.quote(keyword)}&sort=date"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
            }
            
            # ì„¸ì…˜ ì‚¬ìš©ìœ¼ë¡œ ì—°ê²° ìœ ì§€
            session = requests.Session()
            session.headers.update(headers)
            
            response = session.get(search_url, timeout=20)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ê¸°ì‚¬ ì°¾ê¸°
            selectors_to_try = [
                'div.list-type038 li',
                'div.news-list li', 
                'ul.list-basic li',
                'div.search-result-item',
                'article',
                '.item'
            ]
            
            article_elements = []
            for selector in selectors_to_try:
                article_elements = soup.select(selector)
                if article_elements:
                    break
            
            # ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ëª¨ë“  ë§í¬ì—ì„œ ì°¾ê¸°
            if not article_elements:
                all_links = soup.find_all('a', href=True)
                count = 0
                for link in all_links:
                    href = link.get('href', '')
                    text = link.get_text().strip()
                    
                    if ('/view/' in href or '/news/' in href) and len(text) > 15:
                        if keyword.lower() in text.lower():
                            full_url = href if href.startswith('http') else self.base_url + href
                            articles.append({
                                'keyword': keyword,
                                'date': datetime.now().strftime('%Y-%m-%d'),
                                'title': text,
                                'url': full_url,
                                'summary': f"{keyword} ê´€ë ¨ ì—°í•©ë‰´ìŠ¤"
                            })
                            count += 1
                            if count >= max_articles:
                                break
                return articles
            
            # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
            for element in article_elements[:max_articles]:
                try:
                    # ì œëª©ê³¼ ë§í¬
                    title_element = element.select_one('a[href*="/view/"], a[href*="/news/"], .tit-wrap a, a')
                    if not title_element:
                        continue
                        
                    title = title_element.get_text().strip()
                    link = title_element.get('href', '')
                    
                    if not title or len(title) < 10:
                        continue
                    
                    # URL ì™„ì„±
                    if link and not link.startswith('http'):
                        link = self.base_url + link
                    
                    # ë‚ ì§œ
                    date_text = datetime.now().strftime('%Y-%m-%d')
                    date_selectors = ['.info-text01', '.date', '.time', 'time', '.news-date']
                    for selector in date_selectors:
                        date_element = element.select_one(selector)
                        if date_element:
                            date_text = date_element.get_text().strip()
                            break
                    
                    # ìš”ì•½
                    summary = f"{keyword} ê´€ë ¨ ì—°í•©ë‰´ìŠ¤"
                    summary_selectors = ['.lead', '.summary', '.desc', 'p', '.news-summary']
                    for selector in summary_selectors:
                        summary_element = element.select_one(selector)
                        if summary_element:
                            summary_text = summary_element.get_text().strip()
                            if summary_text and len(summary_text) > 10:
                                summary = summary_text[:200] + "..." if len(summary_text) > 200 else summary_text
                                break
                    
                    articles.append({
                        'keyword': keyword,
                        'date': date_text,
                        'title': title,
                        'url': link,
                        'summary': summary
                    })
                    
                except Exception as e:
                    continue
                    
        except requests.exceptions.Timeout:
            st.warning(f"'{keyword}' ê²€ìƒ‰ ìš”ì²­ì´ ì‹œê°„ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except requests.exceptions.RequestException as e:
            st.warning(f"'{keyword}' ê²€ìƒ‰ ì¤‘ ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            st.warning(f"'{keyword}' ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            
        return articles
    
    def get_article_content(self, url):
        """ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ìƒì„¸ ìš”ì•½ìš©)"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
            content_div = soup.find('div', class_='story-news-article')
            if content_div:
                paragraphs = content_div.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs])
                return content[:500] + "..." if len(content) > 500 else content
            
            return "ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        except Exception as e:
            return f"ë³¸ë¬¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}"

# ë©”ì¸ ì•±
def main():
    scraper = YonhapNewsScraper()
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    st.sidebar.header("ê²€ìƒ‰ ì„¤ì •")
    
    # í‚¤ì›Œë“œ ì„ íƒ
    keywords = st.sidebar.multiselect(
        "ê²€ìƒ‰í•  í‚¤ì›Œë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
        ["AI", "í™˜ìœ¨", "ë‚˜ìŠ¤ë‹¥"],
        default=["AI", "í™˜ìœ¨", "ë‚˜ìŠ¤ë‹¥"]
    )
    
    # ê¸°ì‚¬ ìˆ˜ ì„¤ì •
    max_articles = st.sidebar.slider("í‚¤ì›Œë“œë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜", 5, 20, 10)
    
    # ê²€ìƒ‰ ë²„íŠ¼
    if st.sidebar.button("ğŸ” ë‰´ìŠ¤ ê²€ìƒ‰ ì‹œì‘", type="primary"):
        if not keywords:
            st.warning("ìµœì†Œ í•˜ë‚˜ì˜ í‚¤ì›Œë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        all_articles = []
        
        # ì§„í–‰ ìƒí™© í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for i, keyword in enumerate(keywords):
            status_text.text(f"'{keyword}' ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰ ì¤‘...")
            
            articles = scraper.search_news(keyword, max_articles)
            all_articles.extend(articles)
            
            progress_bar.progress((i + 1) / len(keywords))
            time.sleep(1)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
        
        status_text.text("ê²€ìƒ‰ ì™„ë£Œ!")
        
        if all_articles:
            # ê²°ê³¼ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
            df = pd.DataFrame(all_articles)
            
            # ê²°ê³¼ í‘œì‹œ
            st.success(f"ì´ {len(all_articles)}ê°œì˜ ê¸°ì‚¬ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!")
            
            # í‚¤ì›Œë“œë³„ í†µê³„
            st.subheader("ğŸ“Š í‚¤ì›Œë“œë³„ ê¸°ì‚¬ ìˆ˜")
            keyword_counts = df['keyword'].value_counts()
            st.bar_chart(keyword_counts)
            
            # ê¸°ì‚¬ ëª©ë¡ í‘œì‹œ
            st.subheader("ğŸ“° ê²€ìƒ‰ëœ ê¸°ì‚¬ ëª©ë¡")
            
            for keyword in keywords:
                keyword_articles = df[df['keyword'] == keyword]
                if not keyword_articles.empty:
                    st.markdown(f"### ğŸ” '{keyword}' ê´€ë ¨ ê¸°ì‚¬ ({len(keyword_articles)}ê°œ)")
                    
                    for idx, article in keyword_articles.iterrows():
                        with st.expander(f"ğŸ“„ {article['title'][:50]}..."):
                            col1, col2 = st.columns([1, 3])
                            
                            with col1:
                                st.write("**ë‚ ì§œ:**", article['date'])
                                st.write("**í‚¤ì›Œë“œ:**", article['keyword'])
                            
                            with col2:
                                st.write("**ì œëª©:**", article['title'])
                                st.write("**ìš”ì•½:**", article['summary'])
                                st.write("**ë§í¬:**", article['url'])
            
            # CSV ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥
            st.subheader("ğŸ’¾ ë°ì´í„° ë‹¤ìš´ë¡œë“œ")
            csv = df.to_csv(index=False, encoding='utf-8-sig')
            st.download_button(
                label="ğŸ“¥ CSV íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œ",
                data=csv,
                file_name=f"yonhap_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
            
        else:
            st.warning("ê²€ìƒ‰ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¥¼ ì‹œë„í•´ë³´ì„¸ìš”.")

if __name__ == "__main__":
    main()