from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import csv
import time
import re
import logging
from datetime import datetime

# ê¹€í™ì€ ìˆ˜ê°•ìƒ ê²°ê³¼ë¬¼ 

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class DaumNewsCrawlerVisual:
    def __init__(self):
        self.base_url = "https://news.daum.net"
        self.driver = None
        self.setup_driver()
        
    def setup_driver(self):
        """í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì •"""
        chrome_options = Options()
        # ë¸Œë¼ìš°ì €ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì£¼ê¸° ìœ„í•´ headless ëª¨ë“œ ë¹„í™œì„±í™”
        # chrome_options.add_argument("--headless")  # ì£¼ì„ ì²˜ë¦¬í•˜ì—¬ ë¸Œë¼ìš°ì € í‘œì‹œ
        chrome_options.add_argument("--no-sandbox")
        chrome_options.add_argument("--disable-dev-shm-usage")
        chrome_options.add_argument("--disable-gpu")
        chrome_options.add_argument("--window-size=1920,1080")
        chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
        
        try:
            self.driver = webdriver.Chrome(options=chrome_options)
            logger.info("í¬ë¡¬ ë¸Œë¼ìš°ì €ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            logger.error(f"í¬ë¡¬ ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            logger.info("ChromeDriverê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
            raise
        
    def search_ai_news_visual(self, max_articles=15):
        """ì‹œê°ì ìœ¼ë¡œ AI í‚¤ì›Œë“œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        news_list = []
        
        try:
            # 1. ë‹¤ìŒ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì ‘ì†
            logger.info("ğŸŒ ë‹¤ìŒ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ì— ì ‘ì† ì¤‘...")
            self.driver.get(self.base_url)
            time.sleep(3)
            
            # 2. ê²€ìƒ‰ ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­
            logger.info("ğŸ” ê²€ìƒ‰ ë²„íŠ¼ì„ ì°¾ëŠ” ì¤‘...")
            try:
                # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë²„íŠ¼ ì„ íƒì ì‹œë„
                search_selectors = [
                    "a[href*='search']",
                    ".link_search",
                    ".btn_search",
                    "button[type='submit']"
                ]
                
                search_element = None
                for selector in search_selectors:
                    try:
                        search_element = WebDriverWait(self.driver, 5).until(
                            EC.element_to_be_clickable((By.CSS_SELECTOR, selector))
                        )
                        break
                    except:
                        continue
                
                if not search_element:
                    # ì§ì ‘ ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì´ë™
                    logger.info("ğŸ” ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™...")
                    search_url = "https://search.daum.net/search?w=news&q=AI"
                    self.driver.get(search_url)
                else:
                    search_element.click()
                    time.sleep(2)
                    
                    # ê²€ìƒ‰ì–´ ì…ë ¥
                    logger.info("âŒ¨ï¸ 'AI' í‚¤ì›Œë“œ ì…ë ¥ ì¤‘...")
                    search_input = WebDriverWait(self.driver, 10).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, "input[name='q'], input[type='search'], .tf_keyword"))
                    )
                    search_input.clear()
                    search_input.send_keys("AI")
                    search_input.send_keys(Keys.RETURN)
                    
            except Exception as e:
                logger.info("ğŸ” ê²€ìƒ‰ í˜ì´ì§€ë¡œ ì§ì ‘ ì´ë™...")
                search_url = "https://search.daum.net/search?w=news&q=AI"
                self.driver.get(search_url)
            
            time.sleep(3)
            
            # 3. ë‰´ìŠ¤ íƒ­ í´ë¦­ (í•„ìš”í•œ ê²½ìš°)
            try:
                news_tab = self.driver.find_element(By.CSS_SELECTOR, "a[href*='w=news'], .tab_news")
                if news_tab:
                    logger.info("ğŸ“° ë‰´ìŠ¤ íƒ­ í´ë¦­...")
                    news_tab.click()
                    time.sleep(2)
            except:
                pass
            
            # 4. ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ
            logger.info("ğŸ“‹ ë‰´ìŠ¤ ê¸°ì‚¬ ëª©ë¡ ì¶”ì¶œ ì¤‘...")
            
            # í˜ì´ì§€ ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ ë¡œë“œ
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(2)
            
            # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ë‹¤ì–‘í•œ ë‰´ìŠ¤ ì•„ì´í…œ ì„ íƒì ì‹œë„
            news_selectors = [
                '.c-item-doc',
                '.item-title',
                '.news-item',
                '.c-doc',
                'div[data-tiara-layer="news"]'
            ]
            
            news_items = []
            for selector in news_selectors:
                items = soup.select(selector)
                if items:
                    news_items = items
                    logger.info(f"âœ… {len(items)}ê°œì˜ ë‰´ìŠ¤ ì•„ì´í…œì„ '{selector}' ì„ íƒìë¡œ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
                    break
            
            if not news_items:
                # ëª¨ë“  ë§í¬ì—ì„œ ë‰´ìŠ¤ ê´€ë ¨ í•­ëª© ì°¾ê¸°
                all_links = soup.find_all('a', href=True)
                for link in all_links:
                    title = link.get_text(strip=True)
                    url = link.get('href')
                    if title and url and 'AI' in title.upper() and len(title) > 10:
                        if not url.startswith('http'):
                            url = f"https:{url}" if url.startswith('//') else f"https://search.daum.net{url}"
                        news_items.append(link)
            
            # 5. ê° ë‰´ìŠ¤ ì•„ì´í…œì—ì„œ ì •ë³´ ì¶”ì¶œ
            for i, item in enumerate(news_items[:max_articles]):
                try:
                    # ì œëª©ê³¼ ë§í¬ ì¶”ì¶œ
                    title_element = item.find('a') if item.name != 'a' else item
                    if not title_element:
                        continue
                    
                    title = title_element.get_text(strip=True)
                    url = title_element.get('href')
                    
                    if not title or not url or len(title) < 5:
                        continue
                    
                    # URL ì •ë¦¬
                    if not url.startswith('http'):
                        if url.startswith('//'):
                            url = f"https:{url}"
                        elif url.startswith('/'):
                            url = f"https://search.daum.net{url}"
                        else:
                            continue
                    
                    # AI í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
                    if 'AI' in title.upper() or 'ì¸ê³µì§€ëŠ¥' in title:
                        news_list.append({
                            'title': title,
                            'url': url
                        })
                        logger.info(f"ğŸ“„ {len(news_list)}. {title[:50]}...")
                        
                        # ì‹œê°ì  íš¨ê³¼ë¥¼ ìœ„í•œ ë”œë ˆì´
                        time.sleep(0.5)
                        
                except Exception as e:
                    logger.warning(f"ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    continue
            
            logger.info(f"âœ… ì´ {len(news_list)}ê°œì˜ AI ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        return news_list
    
    def get_article_content_and_date_visual(self, url):
        """ì‹œê°ì ìœ¼ë¡œ ê°œë³„ ê¸°ì‚¬ ë‚´ìš©ê³¼ ë‚ ì§œ ì¶”ì¶œ"""
        try:
            logger.info(f"ğŸ“– ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì¤‘: {url[:50]}...")
            
            # ìƒˆ íƒ­ì—ì„œ ê¸°ì‚¬ ì—´ê¸°
            self.driver.execute_script(f"window.open('{url}', '_blank');")
            self.driver.switch_to.window(self.driver.window_handles[-1])
            
            time.sleep(2)
            
            # í˜ì´ì§€ ì†ŒìŠ¤ ê°€ì ¸ì˜¤ê¸°
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            
            # ê¸°ì‚¬ ë‚ ì§œ ì¶”ì¶œ
            date_selectors = [
                'span.num_date',
                'span.date',
                'time',
                'span.txt_date',
                'div.date_area span',
                'span.info_date',
                '.date',
                '.txt_info'
            ]
            
            article_date = ""
            for selector in date_selectors:
                date_element = soup.select_one(selector)
                if date_element:
                    date_text = date_element.get_text(strip=True)
                    date_match = re.search(r'(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})', date_text)
                    if date_match:
                        article_date = date_match.group(1).replace('-', '.').replace('/', '.')
                        break
            
            if not article_date:
                article_date = datetime.now().strftime("%Y.%m.%d")
            
            # ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ
            content_selectors = [
                'div.article_view',
                'div.news_view',
                'div.view_content',
                'div#harmonyContainer',
                'div.news_body',
                'div.article-body',
                '.article_txt',
                '.news_txt'
            ]
            
            content = ""
            for selector in content_selectors:
                content_div = soup.select_one(selector)
                if content_div:
                    for tag in content_div.find_all(['script', 'style', 'iframe', 'ins']):
                        tag.decompose()
                    content = content_div.get_text(strip=True)
                    break
            
            if not content:
                content = soup.get_text()
                content = re.sub(r'\s+', ' ', content).strip()
            
            # íƒ­ ë‹«ê¸°
            self.driver.close()
            self.driver.switch_to.window(self.driver.window_handles[0])
            
            return content[:1000] if content else "ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", article_date
            
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            try:
                self.driver.close()
                self.driver.switch_to.window(self.driver.window_handles[0])
            except:
                pass
            return "ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", datetime.now().strftime("%Y.%m.%d")
    
    def extract_summary_and_keywords(self, title, content):
        """ê°„ë‹¨í•œ ìš”ì•½ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ ìš”ì•½ (ì²« 200ì)
        summary = content[:200] + "..." if len(content) > 200 else content
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ (AI ê´€ë ¨ í‚¤ì›Œë“œ ì°¾ê¸°)
        ai_keywords = ['AI', 'ì¸ê³µì§€ëŠ¥', 'ë¨¸ì‹ ëŸ¬ë‹', 'ë”¥ëŸ¬ë‹', 'ì±—GPT', 'ChatGPT', 
                      'ë¡œë´‡', 'ìë™í™”', 'ë¹…ë°ì´í„°', 'ì•Œê³ ë¦¬ì¦˜', 'ì‹ ê²½ë§']
        
        found_keywords = []
        text_upper = (title + " " + content).upper()
        
        for keyword in ai_keywords:
            if keyword.upper() in text_upper:
                found_keywords.append(keyword)
        
        return summary, ", ".join(found_keywords) if found_keywords else "AI"
    
    def crawl_and_save_visual(self, filename="ai_news_visual.csv", max_articles=15):
        """ì‹œê°ì  ë‰´ìŠ¤ í¬ë¡¤ë§ ë° CSV ì €ì¥"""
        logger.info("ğŸš€ ì‹œê°ì  AI ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œì‘...")
        
        try:
            # ë‰´ìŠ¤ ê²€ìƒ‰
            news_list = self.search_ai_news_visual(max_articles)
            
            if not news_list:
                logger.warning("âŒ ê²€ìƒ‰ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return
            
            # ì¤‘ë³µ ì œê±°
            unique_news = []
            seen_urls = set()
            
            for news in news_list:
                if news['url'] not in seen_urls:
                    unique_news.append(news)
                    seen_urls.add(news['url'])
            
            logger.info(f"âœ… ì´ {len(unique_news)}ê°œì˜ ê³ ìœ í•œ ë‰´ìŠ¤ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            
            # CSV íŒŒì¼ ìƒì„±
            logger.info("ğŸ“Š CSV íŒŒì¼ ìƒì„± ì¤‘...")
            with open(filename, 'w', newline='', encoding='utf-8-sig') as csvfile:
                fieldnames = ['ë²ˆí˜¸', 'ì œëª©', 'URL', 'ë‚ ì§œ', 'ìš”ì•½', 'í‚¤ì›Œë“œ']
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for i, news in enumerate(unique_news, 1):
                    logger.info(f"ğŸ“ ê¸°ì‚¬ {i}/{len(unique_news)} ì²˜ë¦¬ ì¤‘...")
                    
                    # ê¸°ì‚¬ ë‚´ìš©ê³¼ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸°
                    content, article_date = self.get_article_content_and_date_visual(news['url'])
                    
                    # ìš”ì•½ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
                    summary, keywords = self.extract_summary_and_keywords(news['title'], content)
                    
                    # CSVì— ì“°ê¸°
                    writer.writerow({
                        'ë²ˆí˜¸': i,
                        'ì œëª©': news['title'],
                        'URL': news['url'],
                        'ë‚ ì§œ': article_date,
                        'ìš”ì•½': summary,
                        'í‚¤ì›Œë“œ': keywords
                    })
                    
                    logger.info(f"âœ… ì™„ë£Œ: {news['title'][:30]}...")
                    time.sleep(1)  # ì‹œê°ì  íš¨ê³¼ë¥¼ ìœ„í•œ ë”œë ˆì´
            
            logger.info(f"ğŸ‰ í¬ë¡¤ë§ ì™„ë£Œ! {filename} íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        finally:
            self.close_driver()
    
    def close_driver(self):
        """ë¸Œë¼ìš°ì € ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if self.driver:
            logger.info("ğŸ”š ë¸Œë¼ìš°ì €ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤...")
            time.sleep(2)  # ì‚¬ìš©ìê°€ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆë„ë¡ ì ì‹œ ëŒ€ê¸°
            self.driver.quit()
            logger.info("âœ… ë¸Œë¼ìš°ì €ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

def main():
    print("ğŸ¤– AI ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ - ì‹œê°ì  ë²„ì „")
    print("=" * 50)
    
    crawler = DaumNewsCrawlerVisual()
    try:
        crawler.crawl_and_save_visual("ai_news_visual.csv", max_articles=10)
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        crawler.close_driver()

if __name__ == "__main__":
    main()